{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46a49d7-0a92-4cd7-a324-df1348843f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables secup\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import random\n",
    "import torch\n",
    "\n",
    "PATH_PRE=\"/\" #👀👀👀change this to root folder of the code\n",
    "\n",
    "num_gpu = torch.cuda.device_count() # Number of GPUs for prediction\n",
    "CV_fold=5 #set to 0 to train on full data\n",
    "valid_fold=3\n",
    "\n",
    "# ❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤\n",
    "train_data=\"df_ret_long.pkl\"\n",
    "\n",
    "# train_data=f\"df_ret_long_new_{valid_fold}.pkl\"\n",
    "# train_data=f\"df_ret_long_new_{valid_fold}alt.pkl\"\n",
    "# ❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤\n",
    "\n",
    "infer_batch=64\n",
    "infer_max_len=512\n",
    "neg_cnt_1=80\n",
    "neg_cnt_2=40\n",
    "sentence_pooling_method=\"last\" # \"mean\", \"cls\"\n",
    "\n",
    "lora_r=32\n",
    "lora_alpha=64\n",
    "lora_target_modules=\"q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj\"\n",
    "\n",
    "MODEL_VERSION=f\"fold{valid_fold}_round0\"\n",
    "\n",
    "############################## recall modl ##############################\n",
    "MODEL_PATH=f\"{PATH_PRE}/Qwen2.5-14B-Instruct/Qwen2.5-14B-Instruct\"  #👀👀👀change this to where you save the pre-trained model\n",
    "lora_path=f\"none\"\n",
    "\n",
    "############################## rerank modl ##############################\n",
    "rank_model_path=f\"{PATH_PRE}/Qwen2.5-32B-Instruct/Qwen2.5-32B-Instruct\"  #👀👀👀change this to where you save the pre-trained model\n",
    "rank_lora_path=f\"none\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76424922-2e18-4093-a79f-9f479c956b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "# ==============================\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 字典 {MisconceptionId1: embedding1, MisconceptionId2: embedding2, ... } 存于 f\"{PATH_PRE}/data/{MODEL_VERSION}_misc.pkl\"\n",
    "# jsonl文件 格式{'query':row['lora_query'], 'pos':row['MisconceptionId'], 'neg':row['new_hard_recall_pids'][:100]} 存于 f\"{PATH_PRE}/data/{MODEL_VERSION}_recall_top_200.jsonl\"\n",
    "# jsonl文件 格式{'query':query,'pos':pos,'neg':neg,'prompt':\"Given a query with a relevant body,along with a title and abstract of paper,determine whether the paper is pertinent to the query by providing a prediction of either 'Yes' or 'No'.\"}  存于 f\"{PATH_PRE}/data/{MODEL_VERSION}_recall_top_100_for_rank.jsonl\"\n",
    "# pickle文件 valid_df\n",
    "\n",
    "os.chdir(f\"{PATH_PRE}/train\")\n",
    "!python -u get_data.py \\\n",
    "    {PATH_PRE} \\\n",
    "    {MODEL_VERSION} \\\n",
    "    {MODEL_PATH} \\\n",
    "    {train_data} \\\n",
    "    {lora_path} \\\n",
    "    {lora_r} \\\n",
    "    {lora_alpha} \\\n",
    "    {lora_target_modules} \\\n",
    "    {CV_fold} \\\n",
    "    {valid_fold} \\\n",
    "    {num_gpu} \\\n",
    "    {infer_batch} \\\n",
    "    {infer_max_len} \\\n",
    "    {sentence_pooling_method}\\\n",
    "    {neg_cnt_1} \\\n",
    "    {neg_cnt_2} \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0300a5ee-f1c9-4e09-9503-9f06fb50b251",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### TRAIN RECALL MODEL\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "ZERO_STAGE = 2 #DeepSpeed 的 ZeRO (Zero Redundancy Optimizer) 优化器设置的一个参数\n",
    "MODEL_USE = MODEL_VERSION\n",
    "OUTPUT = f\"{PATH_PRE}/model_save/{MODEL_USE}_recall\"\n",
    "os.makedirs(OUTPUT, exist_ok=True)\n",
    "\n",
    "MASTER_PORT = random.randint(10000, 65535)\n",
    "print(f\"Master Port: {MASTER_PORT}\")\n",
    "\n",
    "include_devices = \",\".join(str(i) for i in range(num_gpu))\n",
    "include_param = f\"localhost:{include_devices}\"\n",
    "\n",
    "# effective train batch size = {batch_size}*{num_gpu}*{grad_accumulation_steps}\n",
    "batch_size=8 #一次训练的批次数 #原8\n",
    "grad_accumulation_steps=2 #训练几步更新一次参数\n",
    "train_group_size=4 #一个批次的正负样本总数\n",
    "\n",
    "lora_learning_rate=5e-5 #default 5e-4 #1e-4,5e-5, 2e-5\n",
    "\n",
    "os.chdir(f\"{PATH_PRE}/train\")\n",
    "# 构建命令字符串\n",
    "command = f\"\"\"\n",
    "deepspeed --master_port {MASTER_PORT} --include {include_param} run_recall.py \\\n",
    "       --path_pre {PATH_PRE} \\\n",
    "       --train_data {PATH_PRE}/data/{MODEL_USE}_recall_top_{neg_cnt_1}.jsonl \\\n",
    "       --CV_fold {CV_fold} \\\n",
    "       --valid_fold {valid_fold} \\\n",
    "       --model_name_or_path {MODEL_PATH} \\\n",
    "       --per_device_train_batch_size {batch_size} \\\n",
    "       --per_device_eval_batch_size {batch_size} \\\n",
    "       --train_group_size {train_group_size} \\\n",
    "       --gradient_accumulation_steps {grad_accumulation_steps} \\\n",
    "       --query_max_len 512 \\\n",
    "       --misc_max_len 256 \\\n",
    "       --earystop 0 \\\n",
    "       --eary_stop_epoch 5 \\\n",
    "       --save_batch_steps 20 \\\n",
    "       --save_per_epoch 1 \\\n",
    "       --num_train_epochs 20 \\\n",
    "       --learning_rate 1e-4 \\\n",
    "       --num_warmup_steps 100 \\\n",
    "       --weight_decay 0.01 \\\n",
    "       --lr_scheduler_type cosine \\\n",
    "       --seed 1236 \\\n",
    "       --zero_stage {ZERO_STAGE} \\\n",
    "       --deepspeed \\\n",
    "       --output_dir {OUTPUT} \\\n",
    "       --gradient_checkpointing \\\n",
    "       --lora_learning_rate {lora_learning_rate} \\\n",
    "       --lora_path {lora_path} \\\n",
    "       --lora_r {lora_r} \\\n",
    "       --lora_alpha {lora_alpha} \\\n",
    "       --lora_target_modules {lora_target_modules}\n",
    "\"\"\"\n",
    "\n",
    "# 执行命令\n",
    "os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31797a96-5cf2-4873-b04a-6efde37ac03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### TRAIN RERANK MODEL\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import random\n",
    "import torch\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "num_gpu = torch.cuda.device_count() # Number of GPUs for prediction\n",
    "print(\"GPU count: \", num_gpu)\n",
    "\n",
    "ZERO_STAGE = 2 #DeepSpeed 的 ZeRO (Zero Redundancy Optimizer) 优化器设置的一个参数\n",
    "MODEL_USE = MODEL_VERSION\n",
    "RERANK_OUTPUT = f\"{PATH_PRE}/model_save/{MODEL_USE}_rerank\"\n",
    "os.makedirs(RERANK_OUTPUT, exist_ok=True)\n",
    "\n",
    "# effective train batch size = {batch_size}*{num_gpu}*{grad_accumulation_steps}\n",
    "rank_batch_size=4 #一次训练的批次数\n",
    "rank_grad_accumulation_steps=8 #训练几步更新一次参数\n",
    "rank_train_group_size=4 #一个批次的正负样本总数\n",
    "rank_learning_rate=5e-4 #default 1e-3\n",
    "rank_lora_r=32 #原64\n",
    "rank_lora_alpha=64 #原128\n",
    "rank_lora_target_modules=\"q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj,lm_head\"\n",
    "\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "os.chdir(f\"{PATH_PRE}/train/reranker\")\n",
    "\n",
    "command = f\"\"\"\n",
    "torchrun --nproc_per_node {num_gpu} \\\n",
    "-m run_reranker \\\n",
    "--path_pre {PATH_PRE} \\\n",
    "--output_dir {RERANK_OUTPUT} \\\n",
    "--overwrite_output_dir \\\n",
    "--model_name_or_path {rank_model_path} \\\n",
    "--train_data {PATH_PRE}/data/{MODEL_USE}_recall_top_{neg_cnt_2}_for_rerank.jsonl \\\n",
    "--learning_rate {rank_learning_rate} \\\n",
    "--num_train_epochs 4 \\\n",
    "--per_device_train_batch_size {rank_batch_size} \\\n",
    "--do_train True \\\n",
    "--gradient_accumulation_steps {rank_grad_accumulation_steps} \\\n",
    "--dataloader_drop_last False \\\n",
    "--query_max_len 512 \\\n",
    "--misc_max_len 256 \\\n",
    "--train_group_size {rank_train_group_size} \\\n",
    "--logging_steps 1 \\\n",
    "--save_strategy epoch \\\n",
    "--save_steps 1 \\\n",
    "--save_total_limit 50 \\\n",
    "--ddp_find_unused_parameters False \\\n",
    "--gradient_checkpointing \\\n",
    "--report_to \"none\" \\\n",
    "--warmup_ratio 0.05 \\\n",
    "--bf16 \\\n",
    "--lora_rank {rank_lora_r} \\\n",
    "--lora_alpha {rank_lora_alpha} \\\n",
    "--use_flash_attn False \\\n",
    "--target_modules {rank_lora_target_modules} \\\n",
    "--lora_path {rank_lora_path} \\\n",
    "--deepspeed stage{ZERO_STAGE}.json \\\n",
    "\"\"\"\n",
    "\n",
    "# 执行命令\n",
    "os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81137de0-43fe-4c5b-a6de-27f3b8d70a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWQ\n",
    "\n",
    "import os, glob\n",
    "import math, re\n",
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "from peft import (LoraConfig, get_peft_model, get_peft_model_state_dict)\n",
    "from safetensors.torch import safe_open, load_file, save_file\n",
    "\n",
    "model_path = f\"{PATH_PRE}/Qwen2.5-32B-Instruct/Qwen2.5-32B-Instruct\"  #👀👀👀change this to where you save the pre-trained model\n",
    "lora_path = f\"{PATH_PRE}/model_save/rerank_Qwen_4epochs/checkpoint-332/adapter.bin\"  #👀👀👀change this to the path of the lora you want to merge\n",
    "merged_model_path = f\"{PATH_PRE}/model_save/reranker_merge\" #where merged model params locate\n",
    "quant_path = f\"{PATH_PRE}/model_save/reranker_AWQ\" #where quantized model params locate\n",
    "os.makedirs(merged_model_path, exist_ok=True)\n",
    "os.makedirs(quant_path, exist_ok=True)\n",
    "\n",
    "alpha = 0.8 # wise-ft hyper-params，平衡0-shot和微调，范围[0，1]\n",
    "device=\"cuda:0\"\n",
    "quant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\n",
    "\n",
    "\n",
    "# ⬇️⬇️⬇️ load lora into model and merge and save\n",
    "print(\"Load lora into model and merge and save...\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    ")\n",
    "\n",
    "loraConfig = LoraConfig(\n",
    "        r=32, #32\n",
    "        lora_alpha=64, #64\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\",\"lm_head\"],\n",
    "        bias=\"none\",\n",
    "        lora_dropout=0.05,\n",
    "        task_type=\"CAUSAL_LM\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n",
    "model = get_peft_model(model, loraConfig) # 函数将LoRA设置应用于加载的基础模型，使其能够在微调过程中使用LoRA技术。\n",
    "d = torch.load(lora_path) # 加载LoRA模型的权重。 从指定的 lora_path 加载权重，并确保它们被加载到与模型相同的设备上。\n",
    "\n",
    "# Wise-ft implementation\n",
    "scaled_d = {}\n",
    "for key, value in d.items():\n",
    "    scaled_d[key] = value * math.sqrt(alpha) if \"lora_A\" in key or \"lora_B\" in key else value\n",
    "model.load_state_dict(scaled_d , strict=False) # 将加载的权重应用到模型中。 # strict=False 表示在加载权重时允许模型结构中的某些不匹配。\n",
    "\n",
    "model = model.merge_and_unload()\n",
    "model.save_pretrained(merged_model_path, save_embedding_layers=True)\n",
    "print(\"Trained model bf16 version merging and saving done!\")\n",
    "\n",
    "\n",
    "\n",
    "# ⬇️⬇️⬇️ calibration data prep\n",
    "print(\"calibration data prep...\")\n",
    "sample=128\n",
    "\n",
    "df = pd.read_pickle(f'{PATH_PRE}/data/{train_data}').sample(n=sample, random_state=42)\n",
    "misconceptions = pd.read_csv(f'{PATH_PRE}/data/misconception_mapping.csv').MisconceptionName.values\n",
    "\n",
    "msgs = []\n",
    "for _, row in df.iterrows():\n",
    "    msgs.append([\n",
    "        {\"role\": \"system\", \"content\": \"You are a Mathematics teacher. \"},\n",
    "        {\"role\": \"user\", \"content\": row[\"rerank_query\"] + misconceptions[row[\"MisconceptionId\"]]+\"\\n\\nPlease respond with only 'Yes' or 'No'.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Yes.\"}\n",
    "    ])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "data=[]\n",
    "for msg in msgs:\n",
    "    text = tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=False)\n",
    "    data.append(text.strip())\n",
    "\n",
    "    \n",
    "\n",
    "# ⬇️⬇️⬇️ autoAWQ\n",
    "print(\"AWQ...\")\n",
    "\n",
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load your tokenizer and model with AutoAWQ\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoAWQForCausalLM.from_pretrained(merged_model_path, device_map=\"auto\", safetensors=True)\n",
    "\n",
    "model.quantize(tokenizer, quant_config=quant_config, calib_data=data)\n",
    "model.save_quantized(quant_path, safetensors=True, shard_size=\"4GB\")\n",
    "tokenizer.save_pretrained(quant_path)\n",
    "print(\"model saved!!!!!!!!!!!!!!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_kernel",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
