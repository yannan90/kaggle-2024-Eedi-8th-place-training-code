import pandas as pd
import json
import pickle
from bs4 import BeautifulSoup
import numpy as np
from tqdm.auto import tqdm
from tqdm.autonotebook import trange

import itertools

import torch
import torch.nn.functional as F
from torch import Tensor
from transformers import AutoTokenizer, AutoModel, BitsAndBytesConfig
from peft import (
    LoraConfig,
    get_peft_model,
    get_peft_model_state_dict,
    # prepare_model_for_int8_training,
    set_peft_model_state_dict,
)
import os, gc, sys
import warnings
warnings.filterwarnings('ignore')


def batch_to_device(batch, target_device):
    """
    send a pytorch batch to a device (CPU/GPU)
    """
    for key in batch:
        if isinstance(batch[key], Tensor):
            batch[key] = batch[key].to(target_device)
    return batch

def sentence_embedding(hidden_state, mask, sentence_pooling_method):
    if sentence_pooling_method == 'mean':
        s = torch.sum(hidden_state * mask.unsqueeze(-1).float(), dim=1)
        d = mask.sum(axis=1, keepdim=True).float()
        return s / d
    elif sentence_pooling_method == 'cls':
        return hidden_state[:, 0]
    elif sentence_pooling_method == 'last':
        return last_token_pool(hidden_state, mask)


def last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:
    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])
    if left_padding:
        return last_hidden_states[:, -1]
    else:
        sequence_lengths = attention_mask.sum(dim=1) - 1
        batch_size = last_hidden_states.shape[0]
        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]


def inference(sentences, pids, model, tokenizer, batch_size, max_length, sentence_pooling_method):
    all_embeddings = []
    length_sorted_idx = np.argsort([-len(sen) for sen in sentences])
    sentences_sorted = [sentences[idx] for idx in length_sorted_idx]
    for start_index in trange(0, len(sentences), batch_size, desc="Batches", disable=False):
        sentences_batch = sentences_sorted[start_index: start_index + batch_size]
        features = tokenizer(sentences_batch, max_length=max_length, padding=True, truncation=True, return_tensors="pt")
        features = batch_to_device(features, model.device)
        with torch.no_grad():
            # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
            outputs = model(**features)
            embeddings = sentence_embedding(outputs.last_hidden_state, features['attention_mask'], sentence_pooling_method)
            embeddings = torch.nn.functional.normalize(embeddings , dim=-1)
            # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
            embeddings = embeddings.detach().cpu().numpy().tolist()
        all_embeddings.extend(embeddings)

    all_embeddings = [np.array(all_embeddings[idx]).reshape(1, -1) for idx in np.argsort(length_sorted_idx)]

    sentence_embeddings = np.concatenate(all_embeddings, axis=0)
    result = {pids[i]: em for i, em in enumerate(sentence_embeddings)}
    return result


# ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸
# ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸
# ### å¤šçº¿ç¨‹æ¨ç†
from threading import Thread
from queue import Queue

# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# å¹¶è¡Œæ¨ç† helper funcs
def run_inference(sentences, pids, model, batch_size, max_length, sentence_pooling_method, result_queue, index):
    result = inference(sentences, pids, model[0], model[1], batch_size, max_length, sentence_pooling_method)
    result_queue.put((index, result))  # å°†çº¿ç¨‹ç´¢å¼•ä¸ç»“æœä¸€èµ·å­˜å…¥é˜Ÿåˆ—


# ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸
# ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸ğŸ˜¶â€ğŸŒ«ï¸


if __name__ == '__main__':

    path_pre = sys.argv[1]
    model_version = sys.argv[2]
    model_path = sys.argv[3]
    train_data = sys.argv[4]
    lora_paths = sys.argv[5].split(",")
    weights = [float(x) for x in sys.argv[6].split(",")]
    num_gpu = int(sys.argv[7])
    infer_batch = int(sys.argv[8])
    infer_max_len = int(sys.argv[9])
    sentence_pooling_method = sys.argv[10]
    neg_cnt = int(sys.argv[11])

    loraConfig = LoraConfig(
                r=32,
                lora_alpha=64,
                target_modules=['q_proj','k_proj','v_proj','o_proj','gate_proj','up_proj','down_proj'],
                bias="none",
                lora_dropout=0.05,  # Conventional
                task_type="FEATURE_EXTRACTION",
            )

    # BitsAndBytesConfig ç”¨äºé…ç½®æ¨¡å‹çš„é‡åŒ–å‚æ•°ï¼Œä»¥ä¾¿åœ¨ä½¿ç”¨è¾ƒå°‘çš„å†…å­˜å’Œè®¡ç®—èµ„æºçš„æƒ…å†µä¸‹è¿è¡Œæ¨¡å‹ï¼š
    bnb_config = BitsAndBytesConfig( 
            load_in_4bit=True, #å°†æ¨¡å‹æƒé‡åŠ è½½ä¸º4ä½æ ¼å¼ï¼Œå‡å°‘å†…å­˜å ç”¨ã€‚
            bnb_4bit_use_double_quant=True, #ä½¿ç”¨åŒé‡é‡åŒ–æ–¹æ³•æ¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚
            bnb_4bit_quant_type="nf4", #æŒ‡å®šé‡åŒ–ç±»å‹ä¸º nf4ï¼ˆä¸€ä¸ªç‰¹å®šçš„é‡åŒ–æ ¼å¼ï¼‰ã€‚
            bnb_4bit_compute_dtype=torch.bfloat16 #æŒ‡å®šè®¡ç®—çš„æ•°æ®ç±»å‹ä¸º bfloat16ï¼Œé€šå¸¸ç”¨äºæå‡è®¡ç®—é€Ÿåº¦å’Œå‡å°å†…å­˜å ç”¨ã€‚
        )
    

    use_device = [f'cuda:{i}' for i in range(torch.cuda.device_count())]

    models=[]
    for i in range(len(lora_paths)):

        path=lora_paths[i]
        device=use_device[i%len(use_device)]

        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        model = AutoModel.from_pretrained(model_path, trust_remote_code=True, quantization_config=bnb_config, device_map=device)
        if path !='none':
            print("åŠ è½½ä¹‹å‰çš„loraå‚æ•°...", path)
            model = get_peft_model(model, loraConfig)
            d = torch.load(path, map_location=model.device)
            model.load_state_dict(d, strict=False)
        model = model.eval()
        models.append((model, tokenizer))


    # =============================
    # encoding misconceptions
    # =============================

    df = pd.read_csv(f'{path_pre}/data/misconception_mapping.csv') # data

    sentences = list(df['MisconceptionName'].values)
    pids = list(df['MisconceptionId'].values)

    result_queue = Queue()  # å•ä¸€é˜Ÿåˆ—å­˜å‚¨ç»“æœï¼Œå¸¦ç´¢å¼•ç¡®ä¿é¡ºåº
    threads = []
    for model_index, model in enumerate(models):
        t = Thread(target=run_inference,args=(sentences, pids, model, infer_batch, infer_max_len, sentence_pooling_method, result_queue, model_index))
        threads.append(t)
    for thread in threads: # å¯åŠ¨çº¿ç¨‹
        thread.start()
    for thread in threads: # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        thread.join()

    # æ”¶é›†æ¯ä¸ªçº¿ç¨‹çš„ç»“æœå¹¶æŒ‰ç´¢å¼•æ’åº
    misc_embeds = sorted([result_queue.get() for _ in threads], key=lambda x: x[0])
    misc_embeds = [result[1] for result in misc_embeds]  # æ¯ä¸ªå…ƒç´ æ˜¯dict: {pids[i]: em for i, em in enumerate(sentence_embeddings)}

    # =============================
    # encoding query
    # =============================

    df = pd.read_pickle(f'{path_pre}/data/{train_data}')
    # df = df.head(200) # testing only
    
    print("train_data length: ", len(df))

    sentences = list(df['recall_query'].values)
    pids = list(df['QuestionId_Answer'].values)

    result_queue = Queue()  # å•ä¸€é˜Ÿåˆ—å­˜å‚¨ç»“æœï¼Œå¸¦ç´¢å¼•ç¡®ä¿é¡ºåº
    threads = []
    for model_index, model in enumerate(models):
        t = Thread(target=run_inference,args=(sentences, pids, model, infer_batch, infer_max_len, sentence_pooling_method, result_queue, model_index))
        threads.append(t)
    for thread in threads: # å¯åŠ¨çº¿ç¨‹
        thread.start()
    for thread in threads: # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        thread.join()

    # æ”¶é›†æ¯ä¸ªçº¿ç¨‹çš„ç»“æœå¹¶æŒ‰ç´¢å¼•æ’åº
    qury_embeds = sorted([result_queue.get() for _ in threads], key=lambda x: x[0])
    qury_embeds = [result[1] for result in qury_embeds] # æ¯ä¸ªå…ƒç´ æ˜¯dict: {pids[i]: em for i, em in enumerate(sentence_embeddings)}


    del models, model, tokenizer
    gc.collect()
    torch.cuda.empty_cache()

    # ====================================
    # è·å¾—é«˜æ’åºæ ·æœ¬
    # ====================================

    device = 'cuda:0'

    misc_preds = []
    for _, row in tqdm(df.iterrows()):
        query_id = row['QuestionId_Answer']
        final_scores = None  # Placeholder for accumulating weighted scores
        for i in range(len(qury_embeds)):
            query_em = qury_embeds[i][query_id].reshape(1, -1)
            query_em = torch.tensor(query_em).to(device).view(1, -1)

            misc_em = np.concatenate([e.reshape(1, -1) for e in list(misc_embeds[i].values())])
            misc_em = torch.tensor(misc_em).to(device)
            index_to_misc_index = {index: misc_id for index, misc_id in enumerate(list(misc_embeds[i].keys()))}

            # Calculate cosine similarity
            scores = F.cosine_similarity(query_em, misc_em)

            # Multiply scores by weight for the current i
            weighted_scores = scores * weights[i]

            # Accumulate scores
            final_scores = weighted_scores if final_scores is None else final_scores + weighted_scores

        # Sort the final accumulated scores and get the top 1000 indices
        sort_index = torch.sort(-final_scores).indices.detach().cpu().numpy().tolist()[:1000]

        # Map sorted indices back to misconception IDs
        pids = [index_to_misc_index[index] for index in sort_index]
        misc_preds.append(pids)
    df['top_recall_pids'] = misc_preds

    # ====================================
    # ç”Ÿæˆ hard neg mining çš„è®­ç»ƒæ•°æ®
    # ====================================

    rerank_data = df
    rerank_data = rerank_data[~rerank_data['QuestionId_Answer'].astype(str).str.endswith(('0','1','2','3','4','5'))]

    def filterout_positive(x, y):
        return [xi for xi in x if xi != y]

    rerank_data['new_hard_recall_pids'] = rerank_data.apply(
        lambda row: filterout_positive(row['top_recall_pids'], row['MisconceptionId']), axis=1
    )

    print("train_data for rerank length: ", len(rerank_data))


    # ====================================
    # æå–å‰neg_cntä¸ªhard neg recalls, ç”¨æ¥è®­ç»ƒrerank model
    # ====================================

    cnt = neg_cnt
    next_train = []
    for _,row in rerank_data.iterrows():
        query = row['rerank_query']
        pos = int(row['MisconceptionId'])  # è½¬æ¢ä¸º int ç±»å‹
        negs = [int(neg) for neg in row['new_hard_recall_pids'][:cnt]] 
        next_train.append({'query':query,'pos':pos,'neg':negs,'prompt':"Please respond with only 'Yes' or 'No'."})
    with open(f"{path_pre}/data/{model_version}_recall_top_{cnt}_for_rerank.jsonl", 'w') as f:
        json.dump(next_train, f)