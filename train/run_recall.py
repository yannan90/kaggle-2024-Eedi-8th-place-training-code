#!/usr/bin/env python
# Copyright (c) Microsoft Corporation.
# SPDX-License-Identifier: Apache-2.0

# DeepSpeed Team
import argparse
import json
import time
import os
import math
import sys
import pandas as pd
from torch import nn, Tensor
from tqdm.auto import tqdm, trange
import torch
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler
from torch.utils.data.distributed import DistributedSampler
from transformers import set_seed, AutoConfig, AutoModel, MistralPreTrainedModel, MistralConfig, DynamicCache, \
    Cache
import random
import numpy as np
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    SchedulerType,
    default_data_collator,
    get_scheduler,
    BitsAndBytesConfig
)
from transformers import optimization

import deepspeed
from deepspeed.ops.adam import DeepSpeedCPUAdam, FusedAdam

# from utils.sft_dataset import is_rank_0
import torch.distributed as dist
from peft import (
    LoraConfig,
    get_peft_model,
    get_peft_model_state_dict,
    # prepare_model_for_int8_training,
    set_peft_model_state_dict,
)
from transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask_for_sdpa, \
    _prepare_4d_causal_attention_mask
from transformers.modeling_outputs import BaseModelOutputWithPast
# from transformers.models.mistral.modeling_flax_mistral import MISTRAL_INPUTS_DOCSTRING
from transformers.models.mistral.modeling_mistral import MistralDecoderLayer, MistralRMSNorm
from transformers.utils import add_start_docstrings_to_model_forward

import random
# from peft import prepare_model_for_kbit_training
sys.path.append(
    os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))
from torch.utils.data import Dataset

IGNORE_INDEX = -100


def is_rank_0() -> bool:
    return not dist.is_initialized() or dist.get_rank() == 0


GLOBAL_BATCH_SIZE = 32
MICRO_BATCH_SIZE = 4


def get_train_ds_config(offload,
                        stage=2,
                        enable_hybrid_engine=False,
                        inference_tp_size=1,
                        release_inference_cache=False,
                        pin_parameters=True,
                        tp_gather_partition_size=8,
                        max_out_tokens=512,
                        enable_tensorboard=False,
                        enable_mixed_precision_lora=False,
                        tb_path="",
                        tb_name=""):
    device = "cpu" if offload else "none"
    zero_opt_dict = {
        "stage": stage,
        "offload_param": {
            "device": device
        },
        "offload_optimizer": {
            "device": device
        },
        "stage3_param_persistence_threshold": 1e4,
        "stage3_max_live_parameters": 3e7,
        "stage3_prefetch_bucket_size": 3e7,
        "memory_efficient_linear": False
    }
    if enable_mixed_precision_lora:
        zero_opt_dict["zero_quantized_nontrainable_weights"] = True
        zero_opt_dict["zero_hpz_partition_size"] = torch.cuda.device_count()
    return {
        "train_batch_size": GLOBAL_BATCH_SIZE,
        "train_micro_batch_size_per_gpu": MICRO_BATCH_SIZE,
        "steps_per_print": 10,
        "zero_optimization": zero_opt_dict,
        "fp16": {
            "enabled": True,
            "loss_scale_window": 100
        },
        "gradient_clipping": 1.0,
        "prescale_gradients": False,
        "wall_clock_breakdown": False,
        "hybrid_engine": {
            "enabled": enable_hybrid_engine,
            "max_out_tokens": max_out_tokens,
            "inference_tp_size": inference_tp_size,
            "release_inference_cache": release_inference_cache,
            "pin_parameters": pin_parameters,
            "tp_gather_partition_size": tp_gather_partition_size,
        },
        "tensorboard": {
            "enabled": enable_tensorboard,
            "output_path": f"{tb_path}/ds_tensorboard_logs/",
            "job_name": f"{tb_name}_tensorboard"
        }
    }


def get_optimizer_grouped_parameters(
        model,
        weight_decay,
        lora_lr=5e-4,
        no_decay_name_list=["bias", "LayerNorm.weight"],
        lora_name_list=["lora"],):
    optimizer_grouped_parameters = [
        
        # ç¬¬ä¸€ç»„ï¼šéœ€è¦æƒé‡è¡°å‡çš„å‚æ•°ï¼Œæ’é™¤æŒ‡å®šçš„ no_decay_name_list å’Œ lora_name_list ä¸­çš„å‚æ•°ã€‚
        {
            "params": [
                p for n, p in model.named_parameters()
                if (not any(nd in n for nd in no_decay_name_list)
                    and p.requires_grad and not any(nd in n
                                                    for nd in lora_name_list))
            ],
            "weight_decay":
                weight_decay,
        },
        
        # ç¬¬äºŒç»„ï¼šå±äº LoRA å±‚çš„å‚æ•°ï¼Œå³å‚æ•°åç§°åŒ…å« lora_name_list ä¸­å®šä¹‰çš„å…³é”®å­—çš„å‚æ•°ï¼ŒåŒæ—¶è¿™äº›å‚æ•°éœ€è¦æƒé‡è¡°å‡ï¼Œå¹¶åº”ç”¨ä¸€ä¸ªå•ç‹¬çš„å­¦ä¹ ç‡ lora_lrã€‚
        {
            "params": [
                p for n, p in model.named_parameters()
                if (not any(nd in n for nd in no_decay_name_list)
                    and p.requires_grad and any(nd in n
                                                for nd in lora_name_list))
            ],
            "weight_decay":
                weight_decay,
            "lr":
                lora_lr
        },

        # ç¬¬ä¸‰ç»„ï¼šä¸éœ€è¦æƒé‡è¡°å‡çš„å‚æ•°ï¼Œä¸»è¦æ˜¯ no_decay_name_list ä¸­æŒ‡å®šçš„å‚æ•°ã€‚
        {
            "params": [
                p for n, p in model.named_parameters()
                if (any(nd in n
                        for nd in no_decay_name_list) and p.requires_grad)
            ],
            "weight_decay":
                0.0,
        },
    ]
    if not optimizer_grouped_parameters[1]["params"]:
        optimizer_grouped_parameters.pop(1)
    return optimizer_grouped_parameters


# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
def get_optimizer_only_lora_parameters(
        model,
        weight_decay,
        lora_lr=5e-4,
        no_decay_name_list=["bias", "LayerNorm.weight"],
        lora_name_list=["lora"],):  # ç®€åŒ–ä¸ºåªç”¨ "lora" åŒ¹é…æ‰€æœ‰ LoRA å‚æ•°

    optimizer_grouped_parameters = [
        {
            "params": [
                p for n, p in model.named_parameters()
                if p.requires_grad and any(nd in n for nd in lora_name_list) 
                and not any(nd in n for nd in no_decay_name_list)
            ],
            "weight_decay": weight_decay,
            "lr": lora_lr
        },
        {
            "params": [
                p for n, p in model.named_parameters()
                if p.requires_grad and any(nd in n for nd in lora_name_list)
                and any(nd in n for nd in no_decay_name_list)
            ],
            "weight_decay": 0.0,
            "lr": lora_lr
        },
    ]


    return optimizer_grouped_parameters
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

def print_rank_0(msg, rank=0):
    if rank <= 0:
        print(msg)


def to_device(batch, device):
    output = {}
    for k, v in batch.items():
        try:
            output[k] = v.to(device)
        except:
            output[k] = v
    return output


def set_random_seed(seed):
    if seed is not None:
        set_seed(seed)
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)


import inspect
import math
import os.path
import random
from dataclasses import dataclass
from typing import List, Tuple, Optional, Union

import datasets
from torch.utils.data import Dataset
from transformers import DataCollatorWithPadding, PreTrainedTokenizer

# ==============================
# data => dataset
# ==============================

class TrainDatasetForEmbedding(Dataset):
    def __init__(
            self,
            args,
            tokenizer: PreTrainedTokenizer
    ):
        if os.path.isdir(args.train_data):
            train_datasets = []
            for file in os.listdir(args.train_data):
                temp_dataset = datasets.load_dataset('json', data_files=os.path.join(args.train_data, file),
                                                     split='train')
                if len(temp_dataset) > args.max_example_num_per_dataset:
                    temp_dataset = temp_dataset.select(
                        random.sample(list(range(len(temp_dataset))), args.max_example_num_per_dataset))
                train_datasets.append(temp_dataset)
            self.dataset = datasets.concatenate_datasets(train_datasets)
        else:
            self.dataset = datasets.load_dataset('json', data_files=args.train_data, split='train')

        self.tokenizer = tokenizer
        self.args = args
        print("è®­ç»ƒæ ·æœ¬æ•°é‡: ", len(self.dataset))

        # åŠ è½½misconceptionæ˜ å°„
        df_misc = pd.read_csv(f'{args.path_pre}/data/misconception_mapping.csv') # data
        self.misc_dict = dict(zip(df_misc['MisconceptionId'], df_misc['MisconceptionName']))

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, item) -> Tuple[str, List[str]]:

        '''
        è¿”å› (query, misconceptions(é‡Œé¢æ”¾æ‰€æœ‰çš„misconcpetions, ç¬¬ä¸€ä¸ªæ˜¯posï¼Œåé¢å…¨æ˜¯negs))
        '''
        
         # query: recall_query
        query = self.dataset[item]['query']
        if self.args.query_instruction_for_retrieval is not None:
            query = self.args.query_instruction_for_retrieval + query #åŠ ä¸ªå¼•å¯¼å‰ç¼€ï¼Œä½¿æ¨¡å‹æ›´æ¸…æ¥šåœ°çŸ¥é“æ¯æ®µæ–‡æœ¬çš„è§’è‰²ã€‚

        # misconceptions é‡Œé¢æ”¾æ‰€æœ‰çš„misconcpetions, ç¬¬ä¸€ä¸ªæ˜¯posï¼Œåé¢å…¨æ˜¯negs
        misconceptions = []
        misconceptions.append(self.misc_dict[self.dataset[item]['pos']])
        if len(self.dataset[item]['neg']) < self.args.train_group_size - 1:
            num = math.ceil((self.args.train_group_size - 1) / len(self.dataset[item]['neg']))
            negs = random.sample(self.dataset[item]['neg'] * num, self.args.train_group_size - 1)
        else:
            negs = random.sample(self.dataset[item]['neg'], self.args.train_group_size - 1)
        misconceptions.extend([self.misc_dict[neg] for neg in negs])

        if self.args.misc_instruction_for_retrieval is not None:
            misconceptions = [self.args.misc_instruction_for_retrieval + p for p in misconceptions]
        return query, misconceptions


@dataclass
class EmbedCollator(DataCollatorWithPadding):
    """
    Wrapper that does conversion from List[Tuple[encode_qry, encode_misc]] to List[qry], List[misc]
    and pass batch separately to the actual collator.
    Abstract out data detail for the model.
    """
    query_max_len: int = 512
    misc_max_len: int = 256

    def padding_score(self, teacher_score):
        group_size = None
        for scores in teacher_score:
            if scores is not None:
                group_size = len(scores)
                break
        if group_size is None:
            return None

        padding_scores = [100.0] + [0.0] * (group_size - 1)
        new_teacher_score = []
        for scores in teacher_score:
            if scores is None:
                new_teacher_score.append(padding_scores)
            else:
                new_teacher_score.append(scores)
        return new_teacher_score

    # éšæœºæ©ç æœ‰ 10% çš„æ¦‚ç‡é®ç›– input_ids ä¸­çš„ä¸€éƒ¨åˆ† tokenï¼Œå¹¶å°†å®ƒä»¬æ›¿æ¢ä¸ºç‰¹å®šå€¼ï¼ˆè¿™é‡Œæ˜¯ 2ï¼‰ã€‚å¢å¼ºæ¨¡å‹çš„èƒ½åŠ›ã€‚
    def mask_pad_token(self,q):
        if random.random()>0.9:
            tensor = q['input_ids'].float()
            # åˆ›å»ºä¸€ä¸ªä¸åŸå§‹å¼ é‡å½¢çŠ¶ç›¸åŒçš„éšæœºå¼ é‡
            mask = torch.rand(tensor.shape)

            # è®¾ç½®é˜ˆå€¼ï¼Œå°†å¤§äºé˜ˆå€¼çš„éƒ¨åˆ†è®¾ç½®ä¸º1ï¼Œå°äºé˜ˆå€¼çš„éƒ¨åˆ†è®¾ç½®ä¸º0
            mask = (mask > 0.9).float()

            # ä½¿ç”¨maskå¼ é‡å°†åŸå§‹å¼ é‡ä¸­çš„ä¸€éƒ¨åˆ†å…ƒç´ è®¾ç½®ä¸º2
            tensor = tensor * (1 - mask) + 2 * mask
            tensor = tensor.long()
            q['input_ids'] = tensor
        return q


    def __call__(self, features):
        query = [f[0] for f in features]
        misc = [f[1] for f in features]

        if isinstance(query[0], list):
            query = sum(query, []) # å°†å­åˆ—è¡¨é€ä¸€å±•å¼€å¹¶åˆå¹¶åˆ°ä¸€ä¸ªæ–°çš„åˆ—è¡¨ä¸­ã€‚
        if isinstance(misc[0], list):
            misc = sum(misc, []) # å°†å­åˆ—è¡¨é€ä¸€å±•å¼€å¹¶åˆå¹¶åˆ°ä¸€ä¸ªæ–°çš„åˆ—è¡¨ä¸­ã€‚

        q_collated = self.tokenizer(
            query,
            padding=True,
            truncation=True,
            max_length=self.query_max_len,
            return_tensors="pt",
        )
        q_collated = self.mask_pad_token(q_collated)

        d_collated = self.tokenizer(
            misc,
            padding=True,
            truncation=True,
            max_length=self.misc_max_len,
            return_tensors="pt",
        )
        d_collated = self.mask_pad_token(d_collated)

        # return {"query": q_collated, "misc": d_collated}

        # åªè¿”å›å¿…è¦çš„å­—æ®µï¼Œ å»æ‰labels
        return {
            "query": {
                "input_ids": q_collated["input_ids"],
                "attention_mask": q_collated["attention_mask"],
            },
            "misc": {
                "input_ids": d_collated["input_ids"],
                "attention_mask": d_collated["attention_mask"],
            }
        }



# ==============================
# modeling
# ==============================


class IgnoreLabelsWrapper(nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model
        self.config = model.config
        
        # æ·»åŠ å¿…è¦çš„æ–¹æ³•
        self.prepare_inputs_for_generation = model.prepare_inputs_for_generation
        self.get_encoder = model.get_encoder if hasattr(model, 'get_encoder') else None

        
    def forward(self,labels=None, **kwargs,):
        return self.model(**kwargs)

    def __getattr__(self, name):
        try:
            return super().__getattr__(name)
        except AttributeError:
            return getattr(self.model, name)



# biencoder
class BiEncoderModel(nn.Module):
    def __init__(self,
                 args,
                 # base_model_related **********************************************************************
                 normlized: bool = True,
                 # base_model_related **********************************************************************
                 negatives_cross_device: bool = False,
                 temperature: float = 1.0, # æ§åˆ¶è®¡ç®—ç›¸ä¼¼åº¦æ—¶å¾—åˆ†çš„â€œé”åº¦â€æˆ–â€œå¹³æ»‘åº¦â€ï¼Œè¿™ç§ç”¨æ³•åœ¨å¯¹æ¯”å­¦ä¹ ï¼ˆcontrastive learningï¼‰ä¸­æ¯”è¾ƒå¸¸è§
                 use_inbatch_neg: bool = True,
                 # base_model_related **********************************************************************
                 sentence_pooling_method: str = "last" # å¥å­æ± åŒ–æ–¹æ³•ï¼Œå¯é€‰ "mean", "cls", "last"
                 # base_model_related **********************************************************************
                 ):
        super().__init__()


        # åŸºç¡€æ¨¡å‹å°è£…bnbå’Œlora
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16
        )

        loraConfig = LoraConfig(
            r=args.lora_r,
            lora_alpha=args.lora_alpha,
            target_modules=args.lora_target_modules.split(","),
            bias="none",
            lora_dropout=0.05,  # Conventional
            # task_type="CAUSAL_LM",
            task_type="FEATURE_EXTRACTION",
            inference_mode=False,  # ç¡®ä¿ä¸æ˜¯æ¨ç†æ¨¡å¼
        )
        
        # $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
        ### base_model for encoder
        # model = AutoModel.from_pretrained(args.model_name_or_path, trust_remote_code=True)
        model = AutoModel.from_pretrained(args.model_name_or_path, trust_remote_code=True, quantization_config=bnb_config)
        model = get_peft_model(model, loraConfig)
        if args.lora_path !='none':
            print("åŠ è½½ä¹‹å‰çš„loraå‚æ•°...", args.lora_path)
            d = torch.load(args.lora_path, map_location=model.device)
            model.load_state_dict(d, strict=False)
        self.model = IgnoreLabelsWrapper(model)
        
        # $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
        self.model.print_trainable_parameters()

        # # LLM for query augmentation
        # self.llm = AutoModelForCausalLM.from_pretrained(args.llm_model_name_or_path)

        # self.cross_entropy = nn.CrossEntropyLoss(reduction='mean')
        self.cross_entropy = nn.CrossEntropyLoss(reduction='mean',label_smoothing=0.2) #ğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜Š
        self.normlized = normlized
        self.sentence_pooling_method = sentence_pooling_method
        self.temperature = temperature
        self.use_inbatch_neg = use_inbatch_neg
        self.config = self.model.config

        if not normlized:
            self.temperature = 1.0
            print("reset temperature = 1.0 due to using inner product to compute similarity")
        if normlized:
            if self.temperature > 0.5:
                raise ValueError(
                    "Temperature should be smaller than 1.0 when use cosine similarity (i.e., normlized=True). Recommend to set it 0.01-0.1")

        self.negatives_cross_device = negatives_cross_device
        if self.negatives_cross_device:
            if not dist.is_initialized():
                raise ValueError('Distributed training has not been initialized for representation all gather.')
            #     logger.info("Run in a single GPU, set negatives_cross_device=False")
            #     self.negatives_cross_device = False
            # else:
            self.process_rank = dist.get_rank()
            self.world_size = dist.get_world_size()


    def _dist_gather_tensor(self, t: Optional[torch.Tensor]):
        if t is None:
            return None
            
        # ç¡®ä¿è¾“å…¥æ˜¯è¿ç»­çš„
        t = t.contiguous()
        
        # æ”¶é›†æ‰€æœ‰è¿›ç¨‹çš„å¼ é‡
        all_tensors = [torch.empty_like(t) for _ in range(self.world_size)]
        torch.distributed.all_gather(all_tensors, t)
        
        # å°†æ‰€æœ‰å¼ é‡è¿æ¥èµ·æ¥
        all_tensors[self.process_rank] = t
        all_tensors = torch.cat(all_tensors, dim=0)
        
        return all_tensors


    def gradient_checkpointing_enable(self, **kwargs):
        self.model.gradient_checkpointing_enable(**kwargs)
    
    def last_token_pool(self, last_hidden_states: Tensor,
                        attention_mask: Tensor) -> Tensor:
        left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])
        if left_padding:
            return last_hidden_states[:, -1]
        else:
            sequence_lengths = attention_mask.sum(dim=1) - 1
            batch_size = last_hidden_states.shape[0]
            return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]

    def sentence_embedding(self, hidden_state, mask):
        if self.sentence_pooling_method == 'mean':
            s = torch.sum(hidden_state * mask.unsqueeze(-1).float(), dim=1)
            d = mask.sum(axis=1, keepdim=True).float()
            return s / d
        elif self.sentence_pooling_method == 'cls':
            return hidden_state[:, 0]
        elif self.sentence_pooling_method == 'last':
            return self.last_token_pool(hidden_state, mask)

    def encode(self, features):
        if features is None:
            return None


        device = next(self.model.parameters()).device
        features = {
            'input_ids': features['input_ids'].to(device),
            'attention_mask': features['attention_mask'].to(device)
        }

        out = self.model(**features)
        out_em = self.sentence_embedding(out.last_hidden_state, features['attention_mask'])

        if self.normlized:
            out_em = torch.nn.functional.normalize(out_em, dim=-1)
        return out_em.contiguous()

    def compute_similarity(self, q_em, m_em):
        if len(m_em.size()) == 2:
            return torch.matmul(q_em, m_em.transpose(0, 1))
        return torch.matmul(q_em, m_em.transpose(-2, -1))

    def forward(self, query, misc):
        q_em = self.encode(query)
        m_em = self.encode(misc)

        if self.training:
            if self.negatives_cross_device and self.use_inbatch_neg:
                q_em = self._dist_gather_tensor(q_em)
                m_em = self._dist_gather_tensor(m_em)

            group_size = m_em.size(0) // q_em.size(0) #å¾—åˆ°æ¯ä¸ªqueryå¯¹åº”å¤šå°‘ä¸ªmisc

            if self.use_inbatch_neg:
                scores = self.compute_similarity(q_em, m_em) / self.temperature  # B, B*G
                scores = scores.view(q_em.size(0), -1) # B, B*G

                target = torch.arange(scores.size(0), device=scores.device, dtype=torch.long) #ç”Ÿæˆä» 0 åˆ° scores.size(0)-1 çš„æ•´æ•°åºåˆ—ï¼Œå½¢çŠ¶ä¸º (B,)ã€‚
                target = target * group_size # è¡¨ç¤ºæ¯ä¸ª query çš„æ­£æ ·æœ¬åœ¨åˆ†ç»„ä¸­çš„èµ·å§‹ä½ç½®ã€‚
                loss = self.cross_entropy(scores, target)
            else:
                scores = self.compute_similarity(q_em[:, None, :, ],
                                                 m_em.view(q_em.size(0), group_size, -1)).squeeze(
                    1) / self.temperature

                scores = scores.view(q_em.size(0), -1)
                target = torch.zeros(scores.size(0), device=scores.device, dtype=torch.long) # è¢«è®¾ç½®ä¸ºå…¨é›¶å‘é‡ï¼Œè¡¨ç¤ºæ¯ä¸ªåˆ†ç»„çš„ç¬¬ä¸€ä¸ªæ ·æœ¬æ˜¯æ­£æ ·æœ¬ã€‚è¿™æ„å‘³ç€æ¨¡å‹éœ€è¦å­¦ä¹ å°†åˆ†ç»„ä¸­ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼ˆæ­£æ ·æœ¬ï¼‰çš„å¾—åˆ†è®¾å¾—æ¯”å…¶ä»–è´Ÿæ ·æœ¬é«˜ï¼Œä»è€Œè¾¾åˆ°æ’åºçš„æ•ˆæœã€‚
                loss = self.cross_entropy(scores, target)

        else:
            scores = self.compute_similarity(q_em, m_em)
            loss = None

        return dict(
            scores=scores,
            loss=loss,
            q_em=q_em,
            m_em=m_em,
        )


# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# validation
from eedi_metrics import mapk

def inference(sentences, pids, model, tokenizer, device):
    batch_size = 32
    max_length = 512
    all_embeddings = []
    length_sorted_idx = np.argsort([-len(sen) for sen in sentences])
    sentences_sorted = [sentences[idx] for idx in length_sorted_idx]

    # åˆ†æ‰¹å¤„ç†å¥å­
    for start_index in trange(0, len(sentences), batch_size, desc="Batches", disable=False):
        sentences_batch = sentences_sorted[start_index: start_index + batch_size]
        with torch.no_grad():
            features = tokenizer(sentences_batch, max_length=max_length, padding=True, truncation=True, return_tensors="pt")
            features = to_device(features, device)
            embeddings = model.encode(features).detach()  
            embeddings = embeddings.cpu().numpy().tolist()
        all_embeddings.extend(embeddings)

    all_embeddings = [np.array(all_embeddings[idx]).reshape(1, -1) for idx in np.argsort(length_sorted_idx)]

    sentence_embeddings = np.concatenate(all_embeddings, axis=0)
    result = {pids[i]: em for i, em in enumerate(sentence_embeddings)}
    return result

def evaluation(valid_df, misc_df, model, tokenizer, device):

    model.eval()
    with torch.no_grad():

        query_embeddings = inference(list(valid_df['recall_query'].values), list(valid_df['QuestionId_Answer'].values), model, tokenizer, device)
        misc_embeddings = inference(list(misc_df['MisconceptionName'].values), list(misc_df['MisconceptionId'].values), model, tokenizer, device)
    
    # Ret_topNids = util.semantic_search(query_embeddings, misc_embeddings, top_k=25)
    # valid_df["predicted"] = [[int(x["corpus_id"]) for x in top25id] for top25id in Ret_topNids]
    
    # æå–æ–‡æ¡£åµŒå…¥å¹¶ç”Ÿæˆç´¢å¼•æ˜ å°„
    misc_index_map = {index: doc_id for index, doc_id in enumerate(list(misc_embeddings.keys()))}
    misc_embeddings = np.concatenate([e.reshape(1, -1) for e in list(misc_embeddings.values())])

    predicts = []
    for _, row in tqdm(valid_df.iterrows()):
        query_id = row['QuestionId_Answer']
        query_em = query_embeddings[query_id].reshape(1, -1)
    
        cosine_similarity = np.dot(query_em, misc_embeddings.T).flatten()
    
        sort_index = np.argsort(-cosine_similarity)[:25]
        pids = [misc_index_map[index] for index in sort_index]
        predicts.append(pids)

    truths = [[data] for data in valid_df["MisconceptionId"].values]
    map25 = mapk(truths, predicts)
    recall25 = 0
    for true_ids, pred_ids in zip(truths, predicts):
        if any(tid in pred_ids for tid in true_ids):
            recall25 += 1
    recall25 = recall25 / len(valid_df)
    return map25, recall25
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@


def main():

    # ==============================
    # å¹¶è¡Œé…ç½®
    # ==============================

    print(sys.executable)  # æ‰“å°å½“å‰ç¯å¢ƒçš„ Python è§£æå™¨ç›®å½•
    args = parse_args()

    print(torch.cuda.is_available())
    print(torch.cuda.device_count())

    # local_rank å‚æ•°å†³å®šä½¿ç”¨å“ªä¸ªGPUã€‚å¦‚æœ local_rank ä¸º -1ï¼Œåˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„GPU
    if args.local_rank == -1:
        device = torch.device("cuda")
    else:
        torch.cuda.set_device(args.local_rank)
        device = torch.device("cuda", args.local_rank)
        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs
        # torch.distributed.init_process_group(backend='nccl')
        deepspeed.init_distributed()

    # 3 å½“å‰è¿›ç¨‹çš„å…¨å±€æ’å
    args.global_rank = torch.distributed.get_rank()
    # DeepSpeedé…ç½®: è·å–è®­ç»ƒé…ç½®ï¼Œè®¾ç½®æ‰¹æ¬¡å¤§å°å’Œå…¶ä»–è®­ç»ƒå‚æ•°ã€‚
    ds_config = get_train_ds_config(offload=args.offload,
                                    stage=args.zero_stage,
                                    enable_tensorboard=args.enable_tensorboard,
                                    tb_path=args.tensorboard_path,
                                    tb_name="step1_model")
    ds_config[
        'train_micro_batch_size_per_gpu'] = args.per_device_train_batch_size
    ds_config[
        'train_batch_size'] = args.per_device_train_batch_size * torch.distributed.get_world_size(
    ) * args.gradient_accumulation_steps

    # If passed along, set the training seed now.
    set_random_seed(args.seed)

    torch.distributed.barrier()


    # ==============================
    # è®­ç»ƒå‡†å¤‡
    # ==============================


    #### åˆ†è¯å™¨å’Œæ¨¡å‹åŠ è½½
    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, trust_remote_code=True)
    model = BiEncoderModel(args, normlized=True, negatives_cross_device=True, temperature=0.02)

    train_dataset = TrainDatasetForEmbedding(args, tokenizer)

    #### DataLoaders creation:
    if args.local_rank == -1: #è¿™æ˜¯å•æœºæˆ–å• GPUï¼Œ éšæœºæ‰“ä¹±
        train_sampler = RandomSampler(train_dataset)
    else: #å°†æ•°æ®åˆ’åˆ†ç»™æ¯ä¸ª GPUï¼Œä½¿å¾—æ¯ä¸ªè¿›ç¨‹åªå¤„ç†æ•°æ®é›†çš„ä¸€éƒ¨åˆ†ï¼Œç¡®ä¿å¤šä¸ªè¿›ç¨‹ä¸ä¼šé‡å¤é‡‡æ ·æ•°æ®
        train_sampler = DistributedSampler(train_dataset)

    train_dataloader = DataLoader(train_dataset,
                                  shuffle=(train_sampler is None),
                                  collate_fn=EmbedCollator(
                                      tokenizer,
                                      query_max_len=args.query_max_len,
                                      misc_max_len=args.misc_max_len
                                  ),
                                  sampler=train_sampler,
                                  batch_size=args.per_device_train_batch_size,
                                  pin_memory=True)


    # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
    # load å…¨éƒ¨miscå’Œvalid_dfï¼Œåšvalidationç”¨
    if args.CV_fold>0:
        misc_df = df = pd.read_csv(f'{args.path_pre}/data/misconception_mapping.csv')
        valid_df = pd.read_pickle(f"{args.path_pre}/data/valid_{args.CV_fold}_{args.valid_fold}.pkl")
    # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@


    #### è®¾ç½®éœ€è¦å¾®è°ƒçš„å‚æ•°ä»¥åŠå­¦ä¹ ç‡
    # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
    # # å…¨æ¨¡å‹å¾®è°ƒ
    # optimizer_grouped_parameters = get_optimizer_grouped_parameters(
    #     model, args.weight_decay, args.lora_learning_rate)
    
    # ä»…å¾®è°ƒloraå‚æ•°
    optimizer_grouped_parameters = get_optimizer_only_lora_parameters(
        model, args.weight_decay, args.lora_learning_rate)
    # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

    AdamOptimizer = DeepSpeedCPUAdam if args.offload else FusedAdam
    optimizer = AdamOptimizer(optimizer_grouped_parameters,
                              lr=args.learning_rate,
                              betas=(0.9, 0.95))

    num_update_steps_per_epoch = math.ceil(
        len(train_dataloader) / args.gradient_accumulation_steps)

    max_steps = math.ceil(args.num_train_epochs * num_update_steps_per_epoch)
    lr_scheduler = get_scheduler(
        name=args.lr_scheduler_type,
        optimizer=optimizer,
        num_warmup_steps=math.ceil(max_steps * 0.03) if args.num_warmup_steps == 0 else args.num_warmup_steps,
        num_training_steps=args.num_train_epochs * num_update_steps_per_epoch,
    )

    model, optimizer, _, lr_scheduler = deepspeed.initialize(
        model=model,
        optimizer=optimizer,
        args=args,
        config=ds_config,
        lr_scheduler=lr_scheduler,
        dist_init_required=True)
    

    # ==============================
    # è®­ç»ƒå¼€å§‹
    # ==============================

    # é¦–æ¬¡éªŒè¯
    print_rank_0("***** INITIAL EVALUATION *****", args.global_rank)
    val_map25, val_recall25 = evaluation(valid_df, misc_df, model, tokenizer, device)
    print_rank_0(f"Valid map25: {val_map25}, Hit Rate in Top 25: {val_recall25}", args.global_rank)

    print_rank_0("***** Running training *****", args.global_rank)
    print_rank_0(f"***** Evaluating perplexity, Epoch {0}/{args.num_train_epochs} *****", args.global_rank)

    total_steps = len(train_dataloader) * args.num_train_epochs
    total_loss = 0.
    best_val_map25 = 0. #è®°å½•æœ€ä¼˜éªŒè¯å‚æ•°
    best_val_recall25 = 0. #è®°å½•æœ€ä¼˜éªŒè¯å‚æ•°
    no_improve_epoch = 0.
    global_step = -1
    time_start = time.time()
    cross_entropy = nn.CrossEntropyLoss(reduction='mean')
    for epoch in range(args.num_train_epochs):
        print_rank_0(
            f"Beginning of Epoch {epoch + 1}/{args.num_train_epochs}, Total Batches {len(train_dataloader)}",
            args.global_rank)
        
        for step, batch in enumerate(train_dataloader):
            model.train()
            global_step += 1
            batch = to_device(batch, device)
            loss = model(**batch)['loss']
            model.backward(loss)
            model.step()
            total_loss += loss.item()

        ### log & save

            if global_step % 10 == 0: #æ¯10æ­¥æ‰“å°ä¸€æ¬¡æ—¥å¿—çš„æ¡ä»¶åˆ¤æ–­ã€‚
                time_end = time.time()
                total_time = time_end - time_start  # è®¡ç®—è¿è¡Œæ€»æ—¶é—´
                time_start = time_end
                print_rank_0(
                    f"Beginning of Epoch {epoch + 1}/{args.num_train_epochs}, curr_step:{global_step}/{total_steps} curr_loss {loss.item()} lr:{lr_scheduler.get_last_lr()[0]} use time:{total_time}s",
                    args.global_rank)
            if (global_step + 1) % args.gradient_accumulation_steps == 0: #æ¯ä¸ªæ¢¯åº¦ç´¯ç§¯å‘¨æœŸç»“æŸæ—¶ï¼ˆå³ gradient_accumulation_steps è¾¾åˆ°åï¼‰é‡ç½® total_loss
                loss_mean = total_loss / args.gradient_accumulation_steps  # calculate mean loss
                total_loss = 0.
            
            # å½“ args.save_batch_steps è¢«è®¾ç½®ï¼Œä¸”å½“å‰æ­¥æ•°è¾¾åˆ°æŒ‡å®šçš„é—´éš”æ—¶ï¼Œåœ¨æ‰€æœ‰è¿›ç¨‹ä¸­éƒ½è¿›è¡ŒéªŒè¯å’Œä¿å­˜
            if args.save_batch_steps and (global_step + 1) % args.save_batch_steps == 0 and (is_rank_0() or args.zero_stage == 3):

                print_rank_0(f"***** Evaluating Loss, Epoch {epoch + 1}/{args.num_train_epochs}---Step {global_step}/{total_steps}*****",args.global_rank)
                print_rank_0(f"loss: {loss_mean}", args.global_rank)
                    
                if args.CV_fold>0:
                    # è°ƒç”¨ evaluation è®¡ç®—éªŒè¯æŒ‡æ ‡
                    val_map25, val_recall25 = evaluation(valid_df, misc_df, model, tokenizer, device)
                    print_rank_0(f"Valid map25: {val_map25}, Hit Rate in Top 25: {val_recall25}", args.global_rank)
                    
                    if val_map25 > best_val_map25:
                        print_rank_0(f"val_log----epoch:{epoch},batch:{global_step + 1},save model from {best_val_map25} to {val_map25} !!!",args.global_rank)
                        save_model(args, model, tokenizer, f"best_val_map25_model")
                        best_val_map25 = val_map25
                        no_improve_epoch = 0
                    else:
                        no_improve_epoch += 1
                        print_rank_0(f"val_log----epoch:{epoch},batch:{global_step + 1},no_improve_epoch:{no_improve_epoch},curr_val_map25 {val_map25} best_val_map25 {best_val_map25} ...", args.global_rank)
                    if args.earystop and no_improve_epoch == args.eary_stop_epoch:
                        print_rank_0( f"val_log----epoch:{epoch},batch:{global_step + 1} eary stop,best_val_map25 {best_val_map25} !!!",args.global_rank)
                        return
                    
                    # if val_recall25 > best_val_recall25:
                    #     print_rank_0(f"val_log----epoch:{epoch},batch:{global_step + 1},save model from {best_val_recall25} to {val_recall25} !!!",args.global_rank)
                    #     save_model(args, model, tokenizer, f"best_val_recall25_model")
                    #     best_val_recall25 = val_recall25
                    #     no_improve_epoch = 0
                    # else:
                    #     no_improve_epoch += 1
                    #     print_rank_0(f"val_log----epoch:{epoch},batch:{global_step + 1},no_improve_epoch:{no_improve_epoch},curr_val_recall25 {val_recall25} best_val_recall25 {best_val_recall25} ...", args.global_rank)
                    # if args.earystop and no_improve_epoch == args.eary_stop_epoch:
                    #     print_rank_0( f"val_log----epoch:{epoch},batch:{global_step + 1} eary stop,best_val_recall25 {best_val_recall25} !!!",args.global_rank)
                    #     return
        # ä¿å­˜æ¯ä¸€è½®
        if args.save_per_epoch == 1:
            save_model(args, model, tokenizer, f"epoch_{epoch}_model")
        # ä¿å­˜æœ€åä¸€è½®
        if epoch == args.num_train_epochs - 1:
            save_model(args, model, tokenizer, f"epoch_{epoch}_model")
        model.tput_timer.update_epoch_count()
    
    


def save_model(args, model, tokenizer, sub_folder=None): #åªä¿å­˜tokenizerå’Œloraæƒé‡
    if sub_folder is not None:
        output_dir = os.path.join(args.output_dir, sub_folder)
        print_rank_0('saving model ...', args.global_rank)
        
        # ä¿å­˜ tokenizer
        tokenizer.save_pretrained(output_dir)
        
        # ä»…åœ¨ global_rank ä¸º 0 çš„ä¸»è¿›ç¨‹ä¸Šä¿å­˜æ¨¡å‹
        if args.global_rank == 0:
            
            model_to_save = model.module if hasattr(model, 'module') else model # è·å–æ¨¡å‹çš„å®é™…ç»“æ„ï¼ˆè€ƒè™‘åˆ°å¤šå¡æƒ…å†µï¼‰
            model_to_save = model_to_save.model

            # æ£€æŸ¥å¹¶è§£åŒ… IgnoreLabelsWrapper
            if isinstance(model_to_save, IgnoreLabelsWrapper):
                model_to_save = model_to_save.model

            # æå–å¹¶ä¿å­˜LoRAç›¸å…³çš„æƒé‡
            lora_weights = {k: v for k, v in model_to_save.state_dict().items() if "lora" in k}
            output_lora_file = os.path.join(output_dir, "adapter.bin")
            torch.save(lora_weights, output_lora_file)

        print_rank_0('saving success ...', args.global_rank)

def parse_args():
    parser = argparse.ArgumentParser(
        description=
        "Finetune a transformers model on a causal language modeling task")

    ## Tensorboard logging
    parser.add_argument('--enable_tensorboard',
                        action='store_true',
                        help='Enable tensorboard logging')
    parser.add_argument('--tensorboard_path',
                        type=str,
                        default="step1_tensorboard")

    parser.add_argument('--save_batch_steps', type=int, default=1000)
    parser.add_argument('--earystop', type=bool, default=False)
    parser.add_argument('--eary_stop_epoch', type=int, default=2)
    parser.add_argument('--save_per_epoch', type=int, default=-1)
    parser.add_argument('--path_pre', type=str, default=None)
    parser.add_argument('--train_data', type=str, default=None, help="train data path ")
    parser.add_argument('--CV_fold', type=int, default=0)
    parser.add_argument('--valid_fold', type=int, default=0)
    parser.add_argument('--query_instruction_for_retrieval', type=str, default=None,
                        help="query_instruction_for_retrieval")
    parser.add_argument('--misc_instruction_for_retrieval', type=str, default=None,
                        help="misc_instruction_for_retrieval")
    parser.add_argument('--query_max_len', type=int, default=512)
    parser.add_argument('--misc_max_len', type=int, default=256)
    parser.add_argument('--train_group_size', type=int, default=2, help="train_group_size")
    parser.add_argument(
        "--model_name_or_path",
        type=str,
        help=
        "Path to pretrained model or model identifier from huggingface.co/models.",
        required=True,
    )
    parser.add_argument(
        "--per_device_train_batch_size",
        type=int,
        default=16,
        help="Batch size (per device) for the training dataloader.",
    )
    parser.add_argument(
        "--per_device_eval_batch_size",
        type=int,
        default=16,
        help="Batch size (per device) for the evaluation dataloader.",
    )
    parser.add_argument(
        "--max_seq_len",
        type=int,
        default=512,
        help="The maximum sequence length.",
    )
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=1e-3,
        help=
        "Initial learning rate (after the potential warmup period) to use.",
    )
    parser.add_argument("--weight_decay",
                        type=float,
                        default=0.,
                        help="Weight decay to use.")
    parser.add_argument("--num_train_epochs",
                        type=int,
                        default=1,
                        help="Total number of training epochs to perform.")
    parser.add_argument(
        "--gradient_accumulation_steps",
        type=int,
        default=1,
        help=
        "Number of updates steps to accumulate before performing a backward/update pass.",
    )
    parser.add_argument(
        "--lr_scheduler_type",
        type=SchedulerType,
        default="cosine",
        help="The scheduler type to use.",
        choices=[
            "linear", "cosine", "cosine_with_restarts", "polynomial",
            "constant", "constant_with_warmup"
        ],
    )
    parser.add_argument(
        "--num_warmup_steps",
        type=int,
        default=0,
        help="Number of steps for the warmup in the lr scheduler.")
    parser.add_argument("--output_dir",
                        type=str,
                        default=None,
                        help="Where to store the model.")
    parser.add_argument("--seed",
                        type=int,
                        default=1234,
                        help="A seed for reproducible training.")
    parser.add_argument("--local_rank",
                        type=int,
                        default=-1,
                        help="local_rank for distributed training on gpus")
    parser.add_argument(
        "--lora_learning_rate",
        type=float,
        default=5e-4,
        help=
        "Initial LoRA learning rate (after the potential warmup period) to use."
    )
    parser.add_argument('--gradient_checkpointing',
                        action='store_true',
                        help='Enable HF gradient checkpointing for model.')
    parser.add_argument('--disable_dropout',
                        action='store_true',
                        help='Disable the dropout of the model.')
    # deepspeed features
    parser.add_argument('--offload',
                        action='store_true',
                        help='Enable ZeRO Offload techniques.')
    parser.add_argument(
        '--zero_stage',
        type=int,
        default=0,
        help='ZeRO optimization stage for Actor model (and clones).')
    ## LoRA for efficient training setting
    parser.add_argument("--lora_path", type=str, default='none')
    parser.add_argument("--lora_dim",
                        type=int,
                        default=0,
                        help="If > 0, use LoRA for efficient training.")
    parser.add_argument("--lora_config",
                        type=str,
                        default="./configs/lora_config_llama.json",
                        help="If > 0, use LoRA for efficient training.")
    parser.add_argument("--lora_module_name",
                        type=str,
                        default="decoder.layers.",
                        help="The scope of LoRA.")
    parser.add_argument('--only_optimize_lora',
                        action='store_true',
                        help='Only optimize the LoRA parameters.')
    parser.add_argument('--lora_r',
                        type=int,
                        default=64)
    parser.add_argument('--lora_alpha',
                        type=int,
                        default=128)
    parser.add_argument('--lora_target_modules',
                        type=str,
                        default="q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj")


    parser = deepspeed.add_config_arguments(parser)
    args = parser.parse_args()

    # Validate settings
    if args.gradient_checkpointing and args.lora_dim > 0:
        assert (
            not args.only_optimize_lora
        ), "--gradient_checkpointing and --only_optimize_lora cannot be enabled at the same time."

    return args

if __name__ == "__main__":
    main()