import logging
from dataclasses import dataclass
from typing import Dict, Optional, List, Union

import torch
from torch import nn, Tensor
from transformers import AutoTokenizer
from transformers.file_utils import ModelOutput

logger = logging.getLogger(__name__)

@dataclass
class RerankerOutput(ModelOutput):
    loss: Optional[Tensor] = None
    logits: Optional[Tensor] = None


class BiEncoderModel(nn.Module):
    def __init__(self,
                 model: None,
                 tokenizer: AutoTokenizer = None,
                 train_batch_size: int = 4,
                 ):
        super().__init__()



        self.model = model
        self.tokenizer = tokenizer
        self.cross_entropy = nn.CrossEntropyLoss(reduction='mean',label_smoothing=0.1)
        if self.model.config.pad_token_id is None:
            self.model.config.pad_token_id = self.tokenizer.pad_token_id
        self.config = self.model.config

        self.train_batch_size = train_batch_size

        self.yes_loc = self.tokenizer('Yes', add_special_tokens=False)['input_ids'][-1]

    # ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€
    def _dist_gather_tensor(self, t: Optional[torch.Tensor]):
        if t is None:
            return None
            
        # ç¡®ä¿è¾“å…¥æ˜¯è¿ç»­çš„
        t = t.contiguous()
        
        # æ”¶é›†æ‰€æœ‰è¿›ç¨‹çš„å¼ é‡
        all_tensors = [torch.empty_like(t) for _ in range(self.world_size)]
        torch.distributed.all_gather(all_tensors, t)
        
        # å°†æ‰€æœ‰å¼ é‡è¿æ¥èµ·æ¥
        all_tensors[self.process_rank] = t
        all_tensors = torch.cat(all_tensors, dim=0)
        
        return all_tensors
    # ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€


    def gradient_checkpointing_enable(self, **kwargs):
        self.model.gradient_checkpointing_enable(**kwargs)

    def enable_input_require_grads(self, **kwargs):
        self.model.enable_input_require_grads(**kwargs)

    def encode(self, features):
        if features is None:
            return None
        outputs = self.model(input_ids=features['input_ids'],
                             attention_mask=features['attention_mask'],
                             position_ids=features['position_ids'] if 'position_ids' in features.keys() else None,
                             output_hidden_states=True) # (B, Len, Vocab)
        _, max_indices = torch.max(features['labels'], dim=1) # (B,) # æ¯ä¸ªæ ·æœ¬çš„æœ€å¤§ç±»åˆ«ç´¢å¼•
        predict_indices = max_indices - 1 # (B,)
        logits = [outputs.logits[i, predict_indices[i], :] for i in range(outputs.logits.shape[0])]
        logits = torch.stack(logits, dim=0) # (B, num_vocab) # æ¯ä¸ªæ ·æœ¬çš„å…³é”®ç‚¹ä½çš„é¢„æµ‹è¯åˆ†å¸ƒ
        scores = logits[:, self.yes_loc] # (B, 1) # æ¯ä¸ªæ ·æœ¬çš„å…³é”®ç‚¹ä½çš„yesé¢„æµ‹å€¼

        return scores.contiguous()



    def forward(self, pair: Union[Dict[str, Tensor], List[Dict[str, Tensor]]] = None):  #DataLoaderå¯ä»¥åœ¨æ¨¡å‹forwardä¸­è‡ªè¡Œè§£åŒ…ï¼Œæ ¹æ®é”®åè·å–collated
        ranker_logits = self.encode(pair) # (B*Gï¼Œ1)

        if self.training:
            grouped_logits = ranker_logits.view(self.train_batch_size, -1) # (Bï¼ŒG*1)
            target = torch.zeros(self.train_batch_size, device=grouped_logits.device, dtype=torch.long) # (B,) # è¢«è®¾ç½®ä¸ºå…¨é›¶å‘é‡ï¼Œè¡¨ç¤ºæ¯ä¸ªåˆ†ç»„çš„ç¬¬ä¸€ä¸ªæ ·æœ¬æ˜¯æ­£æ ·æœ¬ã€‚è¿™æ„å‘³ç€æ¨¡å‹éœ€è¦å­¦ä¹ å°†åˆ†ç»„ä¸­ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼ˆæ­£æ ·æœ¬ï¼‰çš„å¾—åˆ†è®¾å¾—æ¯”å…¶ä»–è´Ÿæ ·æœ¬é«˜ï¼Œä»è€Œè¾¾åˆ°æ’åºçš„æ•ˆæœã€‚
            loss = self.compute_loss(grouped_logits, target) #æ ‡é‡
        else:
            loss = None
        return RerankerOutput(loss=loss,logits=ranker_logits,)

    def compute_loss(self, logits, target):
        return self.cross_entropy(logits, target)

    def save(self, output_dir: str):

        self.tokenizer.save_pretrained(output_dir)

        model_to_save = self.model.module if hasattr(self.model, 'module') else self.model # è·å–æ¨¡å‹çš„å®é™…ç»“æ„ï¼ˆè€ƒè™‘åˆ°å¤šå¡æƒ…å†µï¼‰

        lora_weights = {k: v.clone().cpu() for k,v in model_to_save.state_dict().items() if "lora" in k}

        output_lora_file = f"{output_dir}/adapter.bin"
        torch.save(lora_weights, output_lora_file)
